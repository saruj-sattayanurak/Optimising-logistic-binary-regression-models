# Optimising-logistic-binary-regression-models
UCL COMP0197: Applied Deep Learning coursework

Implement a specific logistic model function logistic_fun, that takes input arguments, a weight vector ğ°, a hyperparameter representing the polynomial order ğ‘€, and an input vector representing ğ·-dimensional variable ğ±, and returns the function value representing a positive class probability ğ‘¦.
```
ğ‘¦=ğœ(ğ‘“ğ‘€(ğ±;ğ°))
```
 where ğœ(âˆ™) is the sigmoid function and ğ‘“ğ‘€(ğ±;ğ°) is an ğ‘€-order polynomial function of the input vector ğ±, a linear function of the weight vector ğ±. This function should include all the possible first- and higher-order polynomial terms including interaction terms, for example, when ğ‘€=2 and ğ·=3: 

 ```
 ğ‘“ğ‘€(ğ±;ğ°)=ğ‘¤0+ğ‘¤1ğ‘¥1+ğ‘¤2ğ‘¥2+ğ‘¤3ğ‘¥3+ğ‘¤4ğ‘¥12+ğ‘¤5ğ‘¥22+ğ‘¤6ğ‘¥32+ğ‘¤7ğ‘¥1ğ‘¥2+ğ‘¤8ğ‘¥1ğ‘¥3+ğ‘¤9ğ‘¥2ğ‘¥3
```

where ğ±=[ğ‘¥1,ğ‘¥2,ğ‘¥3]T is and ğ°=[ğ‘¤0,â‹¯ğ‘¤9]T. In general, the total number of polynomial terms is ğ‘=Î£(ğ·+ğ‘šâˆ’1ğ‘š)ğ‘€ğ‘š=0. For the purpose of the coursework, you can define the order of these ğ‘ polynomial terms, as long as it is consistent throughout all the code submitted.

Implement 2 loss functions
* Cross Entropy
* Root Mean Square

Implement a stochastic minibatch gradient descent algorithm for optimising the logistic functions, fit_logistic_sgd, which takes ğ‘ pairs of ğ± and target values ğ‘¡ as input, with additional input arguments, options to choose and minimise one of the above loss functions, learning rate and minibatch size. This function returns the optimum weight vector ğ°Ì‚. During training, the function should report the loss periodically throughout optimisation using printed messages (minimum 10 times including beginning and end).
