=====================================

          COURSEWORK 1 TASK 1
          
=====================================

     SECTION 1: CROSS-ENTROPY LOSS
     
-------------------------------------

  [1.1] CROSS-ENTROPY LOSS with M = 1
  
         ********************
          
Initial Loss = 2.001864433288574

Iteration      Loss      
-------------------------
[Iteration 500] Loss = 0.6652271747589111
[Iteration 1000] Loss = 0.696074903011322
[Iteration 1500] Loss = 0.6298942565917969
[Iteration 2000] Loss = 0.8028866648674011
[Iteration 2500] Loss = 0.6870633363723755
[Iteration 3000] Loss = 0.7168678641319275
[Iteration 3500] Loss = 0.6079391837120056
[Iteration 4000] Loss = 0.639995276927948
[Iteration 4500] Loss = 0.6708674430847168
-------------------------

*** Final Loss = 0.6923681497573853

Optimal weights: tensor([ 0.1656,  0.1106, -0.1551,  0.0150, -0.0100, -0.0082])

> F1-Score on unseen data using true labels (labels without noise) as ground-truth: 54.054054054054056%

> F1-Score on training data using true labels (labels without noise) as ground-truth: 55.072463768115945%

========== OPTIONAL ==========

> F1-Score on unseen data using noisy labels as ground-truth: 56.89655172413792%

> F1-Score on training data using noisy labels as ground-truth: 65.19823788546256%

----------- END OF [1.1] ----------- 

  [1.2] CROSS-ENTROPY LOSS with M = 2
  
         ********************
          
Initial Loss = 11.522879600524902

Iteration      Loss      
-------------------------
[Iteration 500] Loss = 4.61649751663208
[Iteration 1000] Loss = 0.7559708952903748
[Iteration 1500] Loss = 0.6730372905731201
[Iteration 2000] Loss = 0.4599204957485199
[Iteration 2500] Loss = 0.4738743305206299
[Iteration 3000] Loss = 0.5759490132331848
[Iteration 3500] Loss = 0.8137008547782898
[Iteration 4000] Loss = 0.8169552683830261
[Iteration 4500] Loss = 0.8260281682014465
-------------------------

*** Final Loss = 0.6424959897994995

Optimal weights: tensor([-0.6495,  0.1320, -0.1767,  0.0159,  0.0446, -0.0101, -0.0261,  0.0318,
        -0.0400, -0.0400,  0.0057,  0.0264, -0.0510,  0.0255, -0.0521, -0.0012,
        -0.0444, -0.0125,  0.0042,  0.0665,  0.0502])

> F1-Score on unseen data using true labels (labels without noise) as ground-truth: 59.79381443298969%

> F1-Score on training data using true labels (labels without noise) as ground-truth: 67.05882352941177%

========== OPTIONAL ==========

> F1-Score on unseen data using noisy labels as ground-truth: 50.98039215686274%

> F1-Score on training data using noisy labels as ground-truth: 57.89473684210527%

----------- END OF [1.2] ----------- 

  [1.3] CROSS-ENTROPY LOSS with M = 3
  
         ********************
          
Initial Loss = 9.669046401977539

Iteration      Loss      
-------------------------
[Iteration 500] Loss = 10.37862777709961
[Iteration 1000] Loss = 9.213473320007324
[Iteration 1500] Loss = 12.665045738220215
[Iteration 2000] Loss = 10.362421989440918
[Iteration 2500] Loss = 11.512925148010254
[Iteration 3000] Loss = 13.8165922164917
[Iteration 3500] Loss = 8.059123039245605
[Iteration 4000] Loss = 4.6072306632995605
[Iteration 4500] Loss = 11.51368236541748
-------------------------

*** Final Loss = 6.909038543701172

Optimal weights: tensor([-0.9093, -0.1316, -0.9878, -0.6461, -0.5995,  0.0423, -1.1437,  0.3443,
        -0.3608,  0.4066, -0.6818, -1.4917,  0.5449,  1.3051,  0.5350, -1.2620,
        -0.3190,  1.1331,  0.4852,  0.0078,  2.4373,  0.7830,  1.3196, -0.0985,
        -0.1326,  0.7949,  0.4560,  1.3312, -0.3391, -0.5472,  1.2336,  1.4799,
        -0.8853, -0.1581,  1.4335,  0.3749, -0.7190,  0.5781,  0.6469,  0.5008,
         0.9036, -0.3803,  0.6971,  1.7581,  1.5689, -1.5920,  1.1835,  0.1720,
        -0.6300, -1.2276, -1.1172,  0.5840, -0.2487, -0.7572, -1.8291, -0.8131])

> F1-Score on unseen data using true labels (labels without noise) as ground-truth: 50.980392156862756%

> F1-Score on training data using true labels (labels without noise) as ground-truth: 60.317460317460316%

========== OPTIONAL ==========

> F1-Score on unseen data using noisy labels as ground-truth: 52.336448598130836%

> F1-Score on training data using noisy labels as ground-truth: 66.02870813397129%

----------- END OF [1.3] ----------- 

   SECTION 2: ROOT-MEAN-SQUARE LOSS
    
-------------------------------------

[2.1] ROOT-MEAN-SQUARE LOSS with M = 1
 
         ********************
          
Initial Loss = 0.7870386242866516

Iteration      Loss      
-------------------------
[Iteration 500] Loss = 0.520900547504425
[Iteration 1000] Loss = 0.5402294993400574
[Iteration 1500] Loss = 0.6170209050178528
[Iteration 2000] Loss = 0.6082949042320251
[Iteration 2500] Loss = 0.5914085507392883
[Iteration 3000] Loss = 0.6803168058395386
[Iteration 3500] Loss = 0.6662511825561523
[Iteration 4000] Loss = 0.5513474345207214
[Iteration 4500] Loss = 0.7029888033866882
-------------------------

*** Final Loss = 0.630578339099884

Optimal weights: tensor([-0.3495,  1.2975,  0.3407,  0.2082,  0.8166, -0.0776])

> F1-Score on unseen data using true labels (labels without noise) as ground-truth: 44.680851063829785%

> F1-Score on training data using true labels (labels without noise) as ground-truth: 52.27272727272728%

========== OPTIONAL ==========

> F1-Score on unseen data using noisy labels as ground-truth: 56.56565656565656%

> F1-Score on training data using noisy labels as ground-truth: 55.102040816326536%

----------- END OF [2.1] ----------- 

[2.2] ROOT-MEAN-SQUARE LOSS with M = 2
 
         ********************
          
Initial Loss = 0.7681694030761719

Iteration      Loss      
-------------------------
[Iteration 500] Loss = 0.7165887355804443
[Iteration 1000] Loss = 0.6390348076820374
[Iteration 1500] Loss = 0.637654185295105
[Iteration 2000] Loss = 0.7058609127998352
[Iteration 2500] Loss = 0.47808462381362915
[Iteration 3000] Loss = 0.6796302795410156
[Iteration 3500] Loss = 0.7758388519287109
[Iteration 4000] Loss = 0.5989723205566406
[Iteration 4500] Loss = 0.719858705997467
-------------------------

*** Final Loss = 0.6361929178237915

Optimal weights: tensor([ 0.5273,  0.6888, -0.3074, -1.1823, -0.2948,  0.0491,  1.2053,  0.6505,
         0.6010, -0.3837, -0.4081,  0.8303,  0.2843,  0.7154, -0.4466, -0.8492,
         0.4125,  0.8935, -0.2224, -0.2410, -1.5628])

> F1-Score on unseen data using true labels (labels without noise) as ground-truth: 58.94736842105262%

> F1-Score on training data using true labels (labels without noise) as ground-truth: 49.16201117318436%

========== OPTIONAL ==========

> F1-Score on unseen data using noisy labels as ground-truth: 54.0%

> F1-Score on training data using noisy labels as ground-truth: 57.28643216080401%

----------- END OF [2.2] ----------- 

[2.3] ROOT-MEAN-SQUARE LOSS with M = 3
 
         ********************
          
Initial Loss = 0.547722578048706

Iteration      Loss      
-------------------------
[Iteration 500] Loss = 0.5931090116500854
[Iteration 1000] Loss = 0.8071634769439697
[Iteration 1500] Loss = 0.7416504621505737
[Iteration 2000] Loss = 0.741619884967804
[Iteration 2500] Loss = 0.6725292801856995
[Iteration 3000] Loss = 0.7417337894439697
[Iteration 3500] Loss = 0.6709218621253967
[Iteration 4000] Loss = 0.7425379157066345
[Iteration 4500] Loss = 0.7424237132072449
-------------------------

*** Final Loss = 0.6324613094329834

Optimal weights: tensor([-0.4580,  1.7069,  1.2657, -0.3664,  0.2984,  2.3391, -0.5029, -1.1449,
        -0.6446,  0.9077, -0.7470, -0.9147,  0.1357,  0.2098, -0.5500,  0.4078,
         0.8888,  0.2023, -0.2444, -0.2255, -0.3050, -0.0780, -0.9479,  0.7915,
        -0.9906, -0.3201, -0.9533, -0.8626, -0.2523,  0.0997,  1.1393,  0.2059,
         0.7818,  0.1559, -0.7387,  0.1983, -0.6871,  1.0494, -0.3328,  0.6917,
         0.8593, -0.0301, -1.5346, -0.3632, -0.5115, -1.0605,  1.6169,  0.2878,
        -2.9050, -1.6292,  0.2400, -1.1753, -1.3324,  0.0544, -0.8468, -1.1358])

> F1-Score on unseen data using true labels (labels without noise) as ground-truth: 50.980392156862756%

> F1-Score on training data using true labels (labels without noise) as ground-truth: 53.03867403314917%

========== OPTIONAL ==========

> F1-Score on unseen data using noisy labels as ground-truth: 48.598130841121495%

> F1-Score on training data using noisy labels as ground-truth: 56.71641791044776%

----------- END OF [2.3] ----------- 

         SECTION 3: DISCUSSION
         
-------------------------------------

[3.1] WHAT A METRIC (OTHER THAN LOSSES) IS APPROPRIATE FOR THIS CLASSIFICATION PROBLEM? (50 WORDS)

>> CHOOSING F1-SCORE OVER ACCURACY, AS IT'S BETTER FOR IMBALANCED DATASET (PROPORTION OF 0 AND 1 LABELS NOT SIMILAR).
>> WHILE OUR DATASET MAY NOT SIGNIFICANTLY IMBALANCED, REAL-WORLD DATA OFTEN IS. I WANT TO TREAT THIS COURSEWORK AS A
>> REAL-LIFE SCENARIO TO GAIN PRACTICE AND HANDS-ON EXPERIENCE WITH REALISTIC EVALUATION METRICS.

----------- END OF [3.1] ----------- 

[3.2] COMMENT BRIEFLY ON THE METRIC REPORTS (100 WORDS)

>> F1-SCORES ARE HIGHEST WHEN THE HYPERPARAMETER M IS 2, AS IT ALIGNS WITH DATA GENERATION PROCESS.

>> THIS TREND HOLDS FOR BOTH LOSS FUNCTIONS(SEE 1.2 AND 2.2).

>> SINCE THE MODEL WAS TRAINED ON NOISY DATA, F1-SCORES ARE NATURALLY HIGHER WHEN EVALUATED AGAINST NOISY LABELS.

>> AND OBSERVED DATA USUALLY YIELDS HIGHER SCORES THAN UNSEEN.

>> THE MODEL ALSO PERFORMS WELL WITH UNDERLYING TRUE CLASS LABELS(SEE 1.2), SUGGESTING IT CAPTURES PATTERNS, NOT JUST MEMORIZING.

>> WHILE RMS YIELDS LOWER LOSS, CROSS-ENTROPY CONSISTENTLY PRODUCES BETTER F1-SCORES.

>> THIS IS BECAUSE THE LOGISTIC FUNCTIONâ€™S S-CURVE BEHAVIOR (NON-CONSTANT CHANGE RATE) MAKES RMS UNSUITABLE FOR THIS TASK.

----------- END OF [3.2] ----------- 
